{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and loading json file\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation, Dropout\n",
    "# from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'PY1',\n",
       "   'patterns': ['python'],\n",
       "   'responses': ['Is python a programming or scripting language?'],\n",
       "   'options': ['Programming', 'scripting', 'both', 'none'],\n",
       "   'right_key': 'programming',\n",
       "   'wrong_key': 'programming',\n",
       "   'answer': 'programming',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY2',\n",
       "   'patterns': ['programming'],\n",
       "   'responses': ['Is Python case sensitive when dealing with identifiers?'],\n",
       "   'options': ['yes', 'no', 'machine dependent', 'none of the mentioned'],\n",
       "   'right_key': 'case sensitive',\n",
       "   'wrong_key': 'case sensitive',\n",
       "   'answer': 'yes',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY3',\n",
       "   'patterns': ['case sensitive'],\n",
       "   'responses': ['Which of the following symbols are used for comments in Python?'],\n",
       "   'options': ['//', '*/', '/**/', '#'],\n",
       "   'right_key': 'comments',\n",
       "   'wrong_key': 'comments',\n",
       "   'answer': '#',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY4',\n",
       "   'patterns': ['comments'],\n",
       "   'responses': ['Which of the following operators has the highest precedence?'],\n",
       "   'options': ['not', '&', '*', '+'],\n",
       "   'right_key': 'precedence',\n",
       "   'wrong_key': 'precedence',\n",
       "   'answer': '*',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY5',\n",
       "   'patterns': ['precedence'],\n",
       "   'responses': ['Which of the following is not a keyword?'],\n",
       "   'options': ['eval', 'assert', 'nonlocal', 'pass'],\n",
       "   'right_key': 'keyword',\n",
       "   'wrong_key': 'keyword',\n",
       "   'answer': 'eval',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY6',\n",
       "   'patterns': ['keyword'],\n",
       "   'responses': ['Which keyword is used to define methods in Python?'],\n",
       "   'options': ['function', 'def', 'method', 'All of these'],\n",
       "   'right_key': 'method',\n",
       "   'wrong_key': 'method',\n",
       "   'answer': 'def',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY7',\n",
       "   'patterns': ['method'],\n",
       "   'responses': ['Which of these in not a core data type?'],\n",
       "   'options': ['Lists', 'Dictionary', 'Tuples', 'Class'],\n",
       "   'right_key': 'data type',\n",
       "   'wrong_key': 'data type',\n",
       "   'answer': 'Class',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY8',\n",
       "   'patterns': ['data type'],\n",
       "   'responses': ['Which of the following is incorrect file handling mode in Python'],\n",
       "   'options': ['wb+', 'ab', 'xr', 'ab+'],\n",
       "   'right_key': 'file handling',\n",
       "   'wrong_key': 'file handling',\n",
       "   'answer': 'xr',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY9',\n",
       "   'patterns': ['file handling'],\n",
       "   'responses': ['Suppose list1 is [2, 33, 222, 14, 25], What is list1[-1] ?'],\n",
       "   'options': ['error', '25', '33', 'None'],\n",
       "   'right_key': 'list',\n",
       "   'wrong_key': 'list',\n",
       "   'answer': '25',\n",
       "   'r_reward': 10},\n",
       "  {'tag': 'PY10',\n",
       "   'patterns': ['list'],\n",
       "   'responses': ['In order to store values in terms of key and value we use what core data type.'],\n",
       "   'options': ['list', 'tuple', 'class', 'dictionary'],\n",
       "   'right_key': 'end',\n",
       "   'wrong_key': 'end',\n",
       "   'answer': 'dictionary',\n",
       "   'r_reward': 10}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('grp7.json').read()\n",
    "intents = json.loads(data_file)\n",
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PY1', 'PY2', 'PY3', 'PY4', 'PY5', 'PY6', 'PY7', 'PY8', 'PY9', 'PY10']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['python'], 'PY1'),\n",
       " (['programming'], 'PY2'),\n",
       " (['case', 'sensitive'], 'PY3'),\n",
       " (['comments'], 'PY4'),\n",
       " (['precedence'], 'PY5'),\n",
       " (['keyword'], 'PY6'),\n",
       " (['method'], 'PY7'),\n",
       " (['data', 'type'], 'PY8'),\n",
       " (['file', 'handling'], 'PY9'),\n",
       " (['list'], 'PY10')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['case',\n",
       " 'comment',\n",
       " 'data',\n",
       " 'file',\n",
       " 'handling',\n",
       " 'keyword',\n",
       " 'list',\n",
       " 'method',\n",
       " 'precedence',\n",
       " 'programming',\n",
       " 'python',\n",
       " 'sensitive',\n",
       " 'type']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PY1', 'PY10', 'PY2', 'PY3', 'PY4', 'PY5', 'PY6', 'PY7', 'PY8', 'PY9']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort classes        \n",
    "classes = sorted(list(set(classes)))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 documents\n"
     ]
    }
   ],
   "source": [
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 classes ['PY1', 'PY10', 'PY2', 'PY3', 'PY4', 'PY5', 'PY6', 'PY7', 'PY8', 'PY9']\n"
     ]
    }
   ],
   "source": [
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 unique lemmatized words ['case', 'comment', 'data', 'file', 'handling', 'keyword', 'list', 'method', 'precedence', 'programming', 'python', 'sensitive', 'type']\n"
     ]
    }
   ],
   "source": [
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "output_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 1\n",
      "sensitive 0\n",
      "type 0\n",
      "['programming']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 1\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['case', 'sensitive']\n",
      "case 1\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 1\n",
      "type 0\n",
      "['comment']\n",
      "case 0\n",
      "comment 1\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['precedence']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 1\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['keyword']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 1\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['method']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 1\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['data', 'type']\n",
      "case 0\n",
      "comment 0\n",
      "data 1\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 1\n",
      "['file', 'handling']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 1\n",
      "handling 1\n",
      "keyword 0\n",
      "list 0\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n",
      "['list']\n",
      "case 0\n",
      "comment 0\n",
      "data 0\n",
      "file 0\n",
      "handling 0\n",
      "keyword 0\n",
      "list 1\n",
      "method 0\n",
      "precedence 0\n",
      "programming 0\n",
      "python 0\n",
      "sensitive 0\n",
      "type 0\n"
     ]
    }
   ],
   "source": [
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    print(pattern_words)\n",
    "\n",
    "\n",
    "    for w in words:\n",
    "        print(w,1 if w in pattern_words else 0)\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       "        list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [list([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "        list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [list([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       "        list([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       "       [list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       "        list([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "        list([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])],\n",
       "       [list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "        list([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])],\n",
       "       [list([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       "        list([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 1, 8, 0, 6, 3, 2, 4, 7, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "train_svm_y=[]\n",
    "for i in range(len(train_y)):\n",
    "    for j in range(len(train_y[i])):\n",
    "        if train_y[i][j]==0:\n",
    "            count+=1\n",
    "        else:\n",
    "            train_svm_y.append(count)\n",
    "            count=0\n",
    "            break\n",
    "            \n",
    "train_svm_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 35 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "\tclf__C: 0.1\n",
      "\tclf__gamma: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, gamma=0.01)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,KFold\n",
    "\n",
    "pipeline = Pipeline([('clf', SVC(kernel='rbf'))])\n",
    "parameters = {\n",
    "    'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),\n",
    "    'clf__C': (0.1, 0.3, 1, 3, 10, 30,100),\n",
    "}\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=i)\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1,cv=inner_cv, scoring='accuracy')\n",
    "grid_search.fit(train_x, train_svm_y)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "best_c = grid_search.best_params_['clf__C']\n",
    "best_gamma = grid_search.best_params_['clf__gamma']\n",
    "#     predictions = grid_search.predict(X_test)\n",
    "#     print (classification_report(y_test, predictions) )\n",
    "\n",
    "model=SVC(C=best_c,gamma=best_gamma)\n",
    "model.fit(train_x, train_svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# # equal to number of intents to predict output intent with softmax\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# # Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# #fitting and saving the model \n",
    "# hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "# model.save('chatbot_model.h5', hist)\n",
    "\n",
    "# print(\"model created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=model.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# from keras.models import load_model\n",
    "# model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('grp7.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence,model):\n",
    "#     print(sentence)\n",
    "    return_list=[]\n",
    "    p=bow(sentence,words,show_details=False)\n",
    "#     print(np.array([p]))\n",
    "    res=model.predict(np.array([p]))\n",
    "#     print(res.shape)\n",
    "    for i in res:\n",
    "        return_list.append({\"intent\":classes[res[0]]})\n",
    "#         print(return_list)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def getOptions(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['options']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def getRightKeyword(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['right_key']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def getWrongKeyword(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['wrong_key']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def getAnswer(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['answer']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def getReward(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = i['r_reward']\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "answer_list = []\n",
    "reward_list = []\n",
    "user_answer_list = []\n",
    "\n",
    "def getTotalReward(intents_json):\n",
    "    total_reward = []\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        total_reward.append(i['r_reward'])\n",
    "    return sum(total_reward)\n",
    "\n",
    "def chatbot_response(msg,user_answer,reward):\n",
    "    \n",
    "    try:\n",
    "        ints = predict_class(msg, model)\n",
    "        response = getResponse(ints, intents)\n",
    "        options = getOptions(ints, intents)\n",
    "        right_key = getRightKeyword(ints, intents)\n",
    "        wrong_key = getWrongKeyword(ints, intents)\n",
    "        qreward = getReward(ints, intents)\n",
    "        answer = getAnswer(ints, intents)\n",
    "        \n",
    "        #answer_list.append(answer)\n",
    "        reward_list.append(int(reward))\n",
    "    except IndexError:\n",
    "        print ('INVALID KEYWORD')   \n",
    "    if msg == \"no\":\n",
    "        byefn(\"BYE!\")     \n",
    "    elif msg == \"end\":\n",
    "        res = 'Pass'\n",
    "        if sum(reward_list)<70:\n",
    "            res = 'Fail'\n",
    "        byefn('Total Rewards:'+str(sum(reward_list))+'\\nResult: '+res)\n",
    "    else:\n",
    "        try:\n",
    "            button(response,options,right_key,wrong_key,qreward,answer)\n",
    "        except IndexError:\n",
    "            print ('Response Not Found!')\n",
    "            res = 'Pass'\n",
    "            if sum(reward_list)<70:\n",
    "                res = 'Fail'\n",
    "            byefn('Total Rewards:'+str(sum(reward_list))+'\\nResult: '+res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import Tk,Frame,Canvas,Label,Button,BOTTOM\n",
    "\n",
    "root = Tk()\n",
    "root.config(bg='lightblue')\n",
    "root.geometry(\"1000x500\")\n",
    "\n",
    "    \n",
    "f1=Frame(root,bg='lightblue',width=400, height=500, relief='raised', borderwidth=1)\n",
    "f1.pack()\n",
    "Label(f1,text='INTERVIEW',bg='lightblue',fg='black',font=('Times 30 bold')).pack()\n",
    "\n",
    "\n",
    "c1=Canvas(root,bg='lightblue')\n",
    "c1.pack()\n",
    "\n",
    "\n",
    "f2=Frame(root,bg='lightblue')\n",
    "f2.pack()\n",
    "Label(f2,text='Welcome Guest',bg='lightblue',fg='black',font=('Times 20 bold')).pack()\n",
    "\n",
    "\n",
    "f3=Frame(c1,bg='lightblue')\n",
    "f3.grid(row=0,column=0,stick='nsew')\n",
    "f4=Frame(c1,bg='lightblue')\n",
    "f4.grid(row=0,column=0,stick='nsew')\n",
    "f5=Frame(c1,bg='lightblue')\n",
    "f5.grid(row=0,column=0,stick='nsew')\n",
    "\n",
    "\n",
    "def start_ques():\n",
    "    button(\"Are you ready to begin\",[\"Yes\",\"No\"],\"python\",\"no\",answer=\"Yes\")\n",
    "    reward_list = [0]\n",
    "\n",
    "Button(f2,text='START',bd=0,bg='green',font=('Times 30 bold'),fg='black',relief='flat',activebackground='gray',activeforeground='green',command=lambda: start_ques()).pack(side=BOTTOM)\n",
    " \n",
    "    \n",
    "Button(root,text='QUIT',bd=0,bg='green',font=('Times 30 bold'),fg='black',relief='flat',activebackground='gray',activeforeground='green',command=lambda: root.destroy()).pack(side=BOTTOM)\n",
    " \n",
    "\n",
    "def byefn(rew):\n",
    "    f5.tkraise()\n",
    "    nooo=Label(f5,pady=65,text=rew,fg='black',bg='lightblue',font=('Times 30 bold'))\n",
    "    nooo.pack()\n",
    "\n",
    "def button(res,ops,right_key,wrong_key=\"no\",right_reward=0,answer=\"python\"):\n",
    "    f4=Frame(c1,bg='lightblue')\n",
    "    f4.grid(row=0,column=0,stick='nsew')\n",
    "    q1=Label(f4,text=res,fg='black',bg='lightblue',font=('Times 15 bold'))\n",
    "    q1.pack()\n",
    "    ch = 1\n",
    "    for op in ops:\n",
    "        if op == answer:\n",
    "            key = right_key\n",
    "            reward = right_reward\n",
    "        else:\n",
    "            key = wrong_key\n",
    "            reward = 1\n",
    "        button_options(f4,str(ch)+'. '+op,key,reward)\n",
    "        ch += 1\n",
    "    \n",
    "def button_options(fr_no,op,key,reward):\n",
    "    Button(fr_no,text=op,bd=0,fg='green',font=('Times 15 bold'),bg='black',relief='flat',activebackground='gray',activeforeground='green',command=lambda:chatbot_response(key,op,reward)).pack()\n",
    "        \n",
    "root.title('GoFree Passport')\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[7]\n",
    "# x.reshape(-1,1)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table=[]\n",
    "# for r in range(0):\n",
    "#     row=[0]*len(classes)\n",
    "#     for c in range():\n",
    "#         row.append(0)\n",
    "# table.append(row)\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[7]\n",
    "# b=[]\n",
    "# b.append(a)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=len(classes)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res=[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_svm=[[0 for i in range(len(classes))] for j in range(1)]\n",
    "# res_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in res_svm:\n",
    "#     i[res[0]]=1\n",
    "    \n",
    "# res_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa=np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2f0881d01701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'intent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PY1'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mttag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mttag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# c=[{'intent': 'PY1'}]\n",
    "# ttag=c['intent']\n",
    "# ttag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
